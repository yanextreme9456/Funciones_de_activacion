
Este proyecto implementa funciones de activación fundamentales para redes neuronales en Python, incluyendo ReLU, Sigmoide y Tangente Hiperbólica, junto con sus respectivas derivadas. Estas funciones son esenciales en el aprendizaje profundo, ya que permiten la propagación de información y la optimización de modelos mediante retropropagación.

El código está organizado en archivos independientes para cada función de activación, facilitando su reutilización e integración en diferentes proyectos de machine learning. Además, se proporciona un script principal que genera gráficas de las funciones y sus derivadas, lo que permite visualizar su comportamiento.

Para ejecutar el código, simplemente importa las funciones necesarias y llama a la función principal main(), que genera y muestra las gráficas correspondientes. El código hace uso de bibliotecas como NumPy y Matplotlib para cálculos numéricos y visualización de datos.